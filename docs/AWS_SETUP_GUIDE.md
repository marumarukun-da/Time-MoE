# AWS GPUç’°å¢ƒã§ã®Time-MoEæ§‹ç¯‰ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€SSHæŽ¥ç¶šå®Œäº†å¾Œã®AWS GPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã§Time-MoEã®ç’°å¢ƒæ§‹ç¯‰ã‹ã‚‰å‹•ä½œç¢ºèªã¾ã§ã‚’è©³ã—ãèª¬æ˜Žã—ã¾ã™ã€‚

**å‰ææ¡ä»¶**: 
- AWS GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆDeep Learning AMIï¼‰ãŒèµ·å‹•æ¸ˆã¿
- SSHæŽ¥ç¶šãŒå®Œäº†ã—ã¦ã„ã‚‹

## ç›®æ¬¡

1. [ç’°å¢ƒæ§‹ç¯‰](#1-ç’°å¢ƒæ§‹ç¯‰)
2. [Time-MoEã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#2-time-moeã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)
3. [å‹•ä½œç¢ºèª](#3-å‹•ä½œç¢ºèª)
4. [ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°](#4-ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°)

## 1. ç’°å¢ƒæ§‹ç¯‰

### 1.1 ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°

```bash
# ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®æ›´æ–°
sudo apt update && sudo apt upgrade -y
```

### 1.2 CUDAç’°å¢ƒã®ç¢ºèª

```bash
# CUDAãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
nvidia-smi

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4               Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   25C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
```

### 1.3 Pythonç’°å¢ƒã®ç¢ºèªã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
python3 --version
# æœŸå¾…: Python 3.8.x ã¾ãŸã¯ 3.10.x

# pipã®æ›´æ–°
python3 -m pip install --upgrade pip

# ä»®æƒ³ç’°å¢ƒä½œæˆï¼ˆæŽ¨å¥¨ï¼‰
python3 -m venv time_moe_env
source time_moe_env/bin/activate

# ä»®æƒ³ç’°å¢ƒç¢ºèª
which python
# æœŸå¾…: /home/ubuntu/time_moe_env/bin/python
```

### 1.3.1 uvï¼ˆé«˜é€Ÿãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼ï¼‰ã®å°Žå…¥ï¼ˆæŽ¨å¥¨ï¼‰

**uvã¯æ¬¡ä¸–ä»£Pythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã€pipã‚ˆã‚Š10-100å€é«˜é€Ÿã«å‹•ä½œã—ã¾ã™ã€‚**

```bash
# uvã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc

# uvãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
uv --version

# uvã§ä»®æƒ³ç’°å¢ƒä½œæˆï¼ˆå¾“æ¥ã®æ–¹æ³•ã®ä»£æ›¿ï¼‰
uv venv time_moe_env
source time_moe_env/bin/activate

# ä»®æƒ³ç’°å¢ƒç¢ºèª
which python
```

**uvã®ãƒ¡ãƒªãƒƒãƒˆï¼š**
- âš¡ **é«˜é€Ÿæ€§**: pipã‚ˆã‚Š10-100å€é«˜é€Ÿãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- ðŸ”§ **pipäº’æ›**: æ—¢å­˜ã®pipã‚³ãƒžãƒ³ãƒ‰ãŒãã®ã¾ã¾ä½¿ç”¨å¯èƒ½
- ðŸ“¦ **åŠ¹çŽ‡çš„ä¾å­˜é–¢ä¿‚è§£æ±º**: ã‚ˆã‚Šå®‰å®šã—ãŸãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ç®¡ç†

### 1.4 PyTorchã®ç¢ºèª

```bash
# PyTorchã¨CUDAã®å‹•ä½œç¢ºèª
python3 -c "
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'Device count: {torch.cuda.device_count()}')
    print(f'Device name: {torch.cuda.get_device_name(0)}')
"
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹**:
```
PyTorch version: 2.1.0+cu121
CUDA available: True
CUDA version: 12.1
Device count: 1
Device name: Tesla T4
```

## 2. Time-MoEã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

### 2.1 ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³

```bash
# Gitã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
git --version

# Time-MoEãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/marumarukun-da/Time-MoE.git
cd Time-MoE
```

### 2.2 ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

**æ–¹æ³•A: pipä½¿ç”¨ï¼ˆå¾“æ¥ã®æ–¹æ³•ï¼‰**
```bash
# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt

# é€²æ—ç¢ºèªï¼ˆæ•°åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰
echo "ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†"
```

**æ–¹æ³•B: uvä½¿ç”¨ï¼ˆæŽ¨å¥¨ãƒ»é«˜é€Ÿï¼‰**
```bash
# uvã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
uv pip install -r requirements.txt

# é€²æ—ç¢ºèªï¼ˆé€šå¸¸1-2åˆ†ã§å®Œäº†ï¼‰
echo "ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†"
```

### 2.3 Flash Attentionã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæŽ¨å¥¨ï¼‰

**æ–¹æ³•A: pipä½¿ç”¨**
```bash
# äº‹å‰æº–å‚™
pip install packaging ninja

# Flash Attentionã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆ10-15åˆ†ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰
echo "Flash Attentionã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­... æ™‚é–“ãŒã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™"
MAX_JOBS=4 pip install flash-attn==2.6.3 --no-build-isolation
```

**æ–¹æ³•B: uvä½¿ç”¨ï¼ˆæŽ¨å¥¨ãƒ»é«˜é€Ÿï¼‰**
```bash
# äº‹å‰æº–å‚™
uv pip install packaging ninja

# Flash Attentionã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆuvã§ã‚‚é«˜é€ŸåŒ–ï¼‰
echo "Flash Attentionã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­..."
MAX_JOBS=4 uv pip install flash-attn==2.6.3 --no-build-isolation
```

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèªï¼ˆå…±é€šï¼‰**
```bash
python3 -c "
try:
    import flash_attn
    print('Flash Attention ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸ')
except ImportError:
    print('Flash Attention ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•— - ç¶šè¡Œå¯èƒ½')
"
```

### 2.4 ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª

```bash
# Time-MoEå›ºæœ‰ã®ä¾å­˜é–¢ä¿‚ç¢ºèª
python3 -c "
import torch
from transformers import AutoModelForCausalLM
print('PyTorch:', torch.__version__)
print('Transformers ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ')
print('CUDA available:', torch.cuda.is_available())
"
```

## 3. å‹•ä½œç¢ºèª

### 3.1 åŸºæœ¬çš„ãªå‹•ä½œãƒ†ã‚¹ãƒˆ

```bash
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p ~/time_moe_test
cd ~/time_moe_test
```

### 3.2 ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ä½œæˆ

```bash
# Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
cat > create_test_data.py << 'EOF'
import json
import numpy as np

# ã‚µãƒ³ãƒ—ãƒ«æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
np.random.seed(42)
test_data = []

for i in range(10):
    # æ­£å¼¦æ³¢ãƒ™ãƒ¼ã‚¹ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
    t = np.linspace(0, 4*np.pi, 100)
    signal = np.sin(t) + 0.1*np.random.randn(100) + i*0.1
    test_data.append({"sequence": signal.tolist()})

# JSONLå½¢å¼ã§ä¿å­˜
with open('test_data.jsonl', 'w') as f:
    for data in test_data:
        json.dump(data, f)
        f.write('\n')

print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†: {len(test_data)}å€‹ã®æ™‚ç³»åˆ—")
print(f"å„ç³»åˆ—ã®é•·ã•: {len(test_data[0]['sequence'])}")
EOF

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆå®Ÿè¡Œ
python3 create_test_data.py
```

### 3.3 Time-MoEãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œç¢ºèª

```bash
# ãƒ¢ãƒ‡ãƒ«æŽ¨è«–ãƒ†ã‚¹ãƒˆ
cat > test_inference.py << 'EOF'
import torch
from transformers import AutoModelForCausalLM
import json

print("=== Time-MoE å‹•ä½œç¢ºèªãƒ†ã‚¹ãƒˆ ===")

# CUDAç¢ºèª
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
print("\nãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
try:
    model = AutoModelForCausalLM.from_pretrained(
        'Maple728/TimeMoE-50M',
        device_map="auto",
        trust_remote_code=True,
    )
    print("âœ“ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
    print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‡ãƒã‚¤ã‚¹: {next(model.parameters()).device}")
except Exception as e:
    print(f"âœ— ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    exit(1)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("\nãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...")
with open('test_data.jsonl', 'r') as f:
    test_data = [json.loads(line) for line in f]

# æŽ¨è«–ãƒ†ã‚¹ãƒˆ
print("\næŽ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
context_length = 12
test_sequence = test_data[0]['sequence'][:context_length]

# æ­£è¦åŒ–
import numpy as np
seq_array = np.array(test_sequence)
mean = seq_array.mean()
std = seq_array.std()
normed_seq = (seq_array - mean) / std

# ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›
input_tensor = torch.tensor([normed_seq], dtype=torch.float32)
if torch.cuda.is_available():
    input_tensor = input_tensor.cuda()

try:
    # æŽ¨è«–å®Ÿè¡Œ
    with torch.no_grad():
        output = model.generate(input_tensor, max_new_tokens=6)
    
    predictions = output[0, -6:].cpu().numpy()
    # é€†æ­£è¦åŒ–
    denorm_predictions = predictions * std + mean
    
    print("âœ“ æŽ¨è«–å®Ÿè¡ŒæˆåŠŸ")
    print(f"å…¥åŠ›ç³»åˆ—: {test_sequence}")
    print(f"äºˆæ¸¬çµæžœ: {denorm_predictions.tolist()}")
    
except Exception as e:
    print(f"âœ— æŽ¨è«–ã‚¨ãƒ©ãƒ¼: {e}")
    exit(1)

print("\n=== å‹•ä½œç¢ºèªå®Œäº†: ã™ã¹ã¦æ­£å¸¸ ===")
EOF

# æŽ¨è«–ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
cd ~/Time-MoE
python3 ~/time_moe_test/test_inference.py
```

### 3.4 è©•ä¾¡æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ

```bash
# è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ†ã‚¹ãƒˆ
cat > test_evaluation.py << 'EOF'
import pandas as pd
import numpy as np

print("=== è©•ä¾¡æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")

# CSVå½¢å¼ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
np.random.seed(42)
dates = pd.date_range('2023-01-01', periods=200, freq='D')
values = np.cumsum(np.random.randn(200)) + 100

test_df = pd.DataFrame({
    'date': dates,
    'value': values
})

test_df.to_csv('test_eval_data.csv', index=False)
print("âœ“ CSVè©•ä¾¡ãƒ‡ãƒ¼ã‚¿ä½œæˆå®Œäº†")

# è©•ä¾¡å®Ÿè¡Œ
import subprocess
try:
    result = subprocess.run([
        'python3', 'run_eval.py',
        '-d', 'test_eval_data.csv',
        '-p', '10',
        '-c', '50',
        '-b', '1'
    ], capture_output=True, text=True, timeout=300)
    
    if result.returncode == 0:
        print("âœ“ è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡ŒæˆåŠŸ")
        print("å‡ºåŠ›:", result.stdout[-200:])  # æœ€å¾Œã®200æ–‡å­—
    else:
        print(f"è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚¨ãƒ©ãƒ¼: {result.stderr}")
        
except subprocess.TimeoutExpired:
    print("è©•ä¾¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
except Exception as e:
    print(f"è©•ä¾¡å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")

print("=== è©•ä¾¡æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Œäº† ===")
EOF

python3 test_evaluation.py
```

### 3.5 æœ€çµ‚å‹•ä½œç¢ºèª

```bash
# ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç¢ºèª
cat > final_check.py << 'EOF'
import torch
import subprocess
import json
import os

print("=== Time-MoEç’°å¢ƒ æœ€çµ‚ç¢ºèª ===")

checks = []

# 1. CUDAç¢ºèª
cuda_available = torch.cuda.is_available()
checks.append(("CUDA", "âœ“" if cuda_available else "âœ—"))
if cuda_available:
    checks.append(("GPU", torch.cuda.get_device_name(0)))

# 2. ä¾å­˜é–¢ä¿‚ç¢ºèª
try:
    from transformers import AutoModelForCausalLM
    checks.append(("Transformers", "âœ“"))
except ImportError:
    checks.append(("Transformers", "âœ—"))

try:
    import flash_attn
    checks.append(("Flash Attention", "âœ“"))
except ImportError:
    checks.append(("Flash Attention", "âœ— (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)"))

# 3. ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
files_to_check = [
    'main.py',
    'run_eval.py',
    'requirements.txt',
    'docs/FINE_TUNING_GUIDE.md',
    'docs/AWS_SETUP_GUIDE.md'
]

for file in files_to_check:
    exists = os.path.exists(file)
    checks.append((f"File: {file}", "âœ“" if exists else "âœ—"))

# çµæžœè¡¨ç¤º
print("\nç¢ºèªçµæžœ:")
for check, status in checks:
    print(f"{check:20} : {status}")

# GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
if cuda_available:
    print(f"\nGPU ãƒ¡ãƒ¢ãƒª:")
    print(f"ç·å®¹é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"ä½¿ç”¨é‡: {torch.cuda.memory_allocated(0) / 1024**3:.1f} GB")

print("\n=== ç’°å¢ƒç¢ºèªå®Œäº† ===")
EOF

python3 final_check.py
```

## 4. ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### 4.1 ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 4.1.1 CUDAèªè­˜ã•ã‚Œãªã„

**ç—‡çŠ¶**: `torch.cuda.is_available()` ãŒ `False`

**è§£æ±ºæ–¹æ³•**:
```bash
# 1. nvidia-smiã‚³ãƒžãƒ³ãƒ‰ç¢ºèª
nvidia-smi

# 2. GPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ç¢ºèª
# CPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹(t2, t3, m5ç­‰)ã§ã¯GPUã¯ä½¿ç”¨ä¸å¯

# 3. ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo apt update
sudo apt install nvidia-driver-535
sudo reboot
```

#### 4.1.2 ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: `CUDA out of memory`

**è§£æ±ºæ–¹æ³•**:
```bash
# GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
nvidia-smi

# Pythonå®Ÿè¡Œæ™‚ã®ãƒ¡ãƒ¢ãƒªåˆ¶é™
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128

# ã‚ˆã‚Šå°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
# ã‚³ãƒ¼ãƒ‰å†…ã§ batch_size ã‚’å‰Šæ¸›
```

#### 4.1.3 ä¾å­˜é–¢ä¿‚ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: `ModuleNotFoundError` ã‚„ `ImportError`

**è§£æ±ºæ–¹æ³•**:
```bash
# ä»®æƒ³ç’°å¢ƒã®ç¢ºèª
which python
source time_moe_env/bin/activate

# ä¾å­˜é–¢ä¿‚ã®å†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆpipä½¿ç”¨ï¼‰
pip install -r requirements.txt --force-reinstall

# ã¾ãŸã¯ uvä½¿ç”¨ï¼ˆæŽ¨å¥¨ãƒ»é«˜é€Ÿï¼‰
uv pip install -r requirements.txt --force-reinstall

# transformersãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
pip show transformers
# ã¾ãŸã¯
uv pip show transformers

# ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒ4.40.1ã§ãªã„å ´åˆï¼ˆpipï¼‰
pip install transformers==4.40.1 --force-reinstall
# ã¾ãŸã¯ uvä½¿ç”¨
uv pip install transformers==4.40.1 --force-reinstall
```

#### 4.1.4 ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æŽ¥ç¶šã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**: ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒå¤±æ•—

**è§£æ±ºæ–¹æ³•**:
```bash
# ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæŽ¥ç¶šç¢ºèª
ping google.com

# Hugging Face Hubã®æŽ¥ç¶šç¢ºèª
python3 -c "from transformers import AutoTokenizer; print('OK')"

# ãƒ—ãƒ­ã‚­ã‚·è¨­å®šãŒå¿…è¦ãªå ´åˆ
export https_proxy=http://proxy-server:port
export http_proxy=http://proxy-server:port
```

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

ç’°å¢ƒæ§‹ç¯‰ãŒå®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ï¼š

- **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰**: `docs/FINE_TUNING_GUIDE.md`
- **CLAUDE.md**: é–‹ç™ºæ™‚ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹

---

ðŸŽ‰ **ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼** Time-MoEã®AWS GPUç’°å¢ƒæ§‹ç¯‰ãŒå®Œäº†ã—ã¾ã—ãŸã€‚

æ¬¡ã¯ `docs/FINE_TUNING_GUIDE.md` ã‚’å‚ç…§ã—ã¦ã€ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆã¯ã€[Time-MoE GitHub Issues](https://github.com/marumarukun-da/Time-MoE/issues) ã§è³ªå•ã—ã¦ãã ã•ã„ã€‚